\chapter{Results}

\section{Term Statistics}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{graphs/idf-scores}
    \caption{IDF scores for terms in the annotations}
    \label{fig:idf-scores}
\end{figure}

Concepts chosen for relevance assessment ad hears to Zipf's law. The graph in Figure \ref{fig:idf-scores} illustrates the IDF scores for each of the concepts in the corpus of annotations. Just 40 concepts are shown out of a total of 2041. 

Note that the notion of concepts are different to terms since a concept can contain more than once word (this is possible through tags, where a tag can contain multiple words such as `shopping mall' and `street sign').

\FloatBarrier
\section{Annotation Statistics}

In total, five annotators managed to annotate a total of \todo{10,982} images across all annotation methodologies. Figure \ref{fig:annotator-breakdown} illustrates the number of annotations completed by each annotator. Two annotators accounted for the majority of the annotations, while three others provide an additional \todo{904}. The exact number of each annotation type, the total time it took to annotate each annotation type and the average time it took to annotate is shown in Table \ref{table:annotation-stats}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/annotator-breakdown}
    \caption{Total number of annotations by annotator}
    \label{fig:annotator-breakdown}
\end{figure}

\begin{table}[!htb]
    \centering
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    Name & Count & Average Time & Total Time \\ \hline
    Text & 3172 & 1 minute & 2 days, 23 hours \\ \hline
    Tag & 2897 & 31 seconds & 23 hours, 40 minutes \\ \hline
    Query & 3616 & 16 seconds & 15 hours, 10 minutes \\ \hline
    Assessment & 1327 & 58 seconds & 21 hours, 36 minutes \\ \hline
    \end{tabular}
    \caption{Annotation Statistics}
    \label{table:annotation-stats}
\end{table}

Textual annotations accounted for the most amount of time in the collection process by far. The most number of annotations collected for a methodology were the query annotations. Accounts from annotators noted that the relevance assessments were the most difficult to collect. 

\FloatBarrier
\section{Retrieval Effectiveness}

Each experiment is displayed as a table which provides the results from \verb|trec_eval| and as a precision-recall graph. The results from only the manually annotated images are displayed first. Table \ref{table:manual-results} display the \verb|trec_eval| results for the four annotation methodologies and the result of combining all four of the methodologies. These results are visualised as a precision-recall graph in Figure \ref{fig:manual-result}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         Methodology & MAP & RR & P@10 & Relevant Retrieved \\ \hline
         Text & 0.3442 & 0.9223 & 0.8333 & 1062 \\ \hline
         Tag & 0.5468 & 0.9578 & 0.8396 & 1040 \\ \hline
         Query & 0.6400 & 0.9653 & 0.8750 & 1559 \\ \hline
         Relevance Assessment & 0.5649 & 0.9504 & 0.8194 & 695 \\ \hline
         All & 0.6512 & 0.0393 & 0.0169 & 1605 \\ \hline
    \end{tabular}
    \caption{MAP, Reciprocal Rank, and Precision at 10 scores for the manual annotations}
    \label{table:manual-results}
\end{table}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/manual-result}
    \caption{Precision-recall curves for the manual annotations}
    \label{fig:manual-result}
\end{figure}

\FloatBarrier

A neural network was trained on the manual textual, tag, and query annotations. The network was able to produce captions for every image in the collection. The automatic captions that were generated were then evaluated in the same way as the manual annotations. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         Methodology & MAP & RR & P@10 & Relevant Retrieved\\ \hline
         Text & 0.0048 & 0.0248 & 0.0196 & 187 \\ \hline
         Tag & 0.0083 & 0.0184 & 0.022 & 136 \\ \hline
         Query & 0.0076 & 0.0174 & 0.0021 & 246 \\ \hline
         All & 0.0164 & 0.0393 & 0.0169 & 337 \\ \hline
    \end{tabular}
    \caption{MAP, Reciprocal Rank, and Precision at 10 scores for the learnt annotations}
    \label{table:learnt-results}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/learnt-result}
    \caption{Precision-recall curves for the learnt annotations}
    \label{fig:manual-result}
\end{figure}

\FloatBarrier
There is no clear individual annotation methodology that outperforms the others, the scores are too low to indicate this. Combining the three automatic annotations together, however, does seem to increase the overall precision.

The number of iterations the neural network covered for each annotation methodology is visualised in Figures \ref{fig:val-loss-1} and \ref{fig:val-loss-2}. The results of the automatic captioning do not get better over time --- in fact they get worse.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/initial-validation-loss-history}
    \caption{Validation loss history (\textless 100,000 iterations)}
    \label{fig:val-loss-1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/validation-loss-history}
    \caption{Validation loss history (\textgreater 200,000 iterations)}
    \label{fig:val-loss-2}
\end{figure}
