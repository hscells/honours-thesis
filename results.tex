\chapter{Results}

\section{Term Statistics}

- display tf and maybe idf graph and table (one with stopwords and one without stopword)

\includegraphics[width=0.95\textwidth]{images/graphs/idf}

- in total, there were `x` experts annotating images, however not all of these annotators were able to annotate the same number of images. The number of images annotated for each type of annoation methodology is displayed below


\section{Annotation Statistics}

\begin{table}[htb]
    \centering
    \begin{adjustbox}{center}
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    & Text & Tags & Query & Relevance Assessment \\ \hline
    Count & 3172 & 2897 & 3616 & 1305 \\ \hline
    Average Time & 1 minute & 31 seconds & 16 seconds & 58 seconds \\ \hline
    Total Time & 2 days, 23 hours & 23 hours, 40 minutes & 15 hours, 10 minutes & 21 hours, 23 minutes \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Annotation Statistics}
\end{table}

text
Textual annotation are the most time consuming annotation type to collect. Many images contain two or more highly relevant events or `important' objects. Describing these can take annotators up to several minutes each. This is opposed to all the other annotation methodologies where most images can be annotated in a minute or less.

tags
In terms of evaluation, tags may be the opposite of textual annotations; intuitively tags should be the best at training an image classifier and are expected to perform the worst when embedded a search task. Tags, however, may be very good at boosting the performance of other annotation types in the search task when combined. For instance, searching on the text \textit{and} tags fields may increase the scores of text annotation alone.

query
and therefore it is unknown how well the collected queries will perform in search or for training an image classifier. The level of detail in these annotations will be very low, since most queries should be short in length.
Much like tags, this annotation type will be very easy to collect. Formulating a query for an image should not takz a significant amount of time. There is the problem of bias \todo{Is there really? I should investigate this further}

\section{Retrieval Effectiveness}

- the results of each experiment are detailed below. first, each individual methodology, followed by each combination.\\
- tables of results go here\\
- outline the most effective combination here\\
- use of pre-trained model (Karpathy) for automatically captioning image had no impact on the retrieval performance.

\begin{table}[htb]
    \parbox{.45\linewidth}{
    \begin{tabular}{ | l | l | }
    \hline
    Metric & Score \\ \hline
	num\_q & 48 \\ \hline
	num\_ret & 27382 \\ \hline
	num\_rel & 6708 \\ \hline
	num\_rel\_ret & 1062 \\ \hline
	map & 0.5436 \\ \hline
	gm\_map & 0.3442 \\ \hline
	Rprec & 0.5506 \\ \hline
	bpref & 0.5564 \\ \hline
	recip\_rank & 0.9223 \\ \hline
	iprec\_at\_recall\_0.00 & 0.9596 \\ \hline
	iprec\_at\_recall\_0.10 & 0.8213 \\ \hline
	iprec\_at\_recall\_0.20 & 0.6831 \\ \hline
	iprec\_at\_recall\_0.30 & 0.5999 \\ \hline
	iprec\_at\_recall\_0.40 & 0.5567 \\ \hline
	iprec\_at\_recall\_0.50 & 0.5296 \\ \hline
	iprec\_at\_recall\_0.60 & 0.5236 \\ \hline
	iprec\_at\_recall\_0.70 & 0.4827 \\ \hline
	iprec\_at\_recall\_0.80 & 0.3974 \\ \hline
	iprec\_at\_recall\_0.90 & 0.2895 \\ \hline
	iprec\_at\_recall\_1.00 & 0.2458 \\ \hline
	P\_5 & 0.8833 \\ \hline
	P\_10 & 0.8333 \\ \hline
	P\_15 & 0.7778 \\ \hline
	P\_20 & 0.7365 \\ \hline
	P\_30 & 0.6437 \\ \hline
	P\_100 & 0.2150 \\ \hline
	P\_200 & 0.1083 \\ \hline
	P\_500 & 0.0434 \\ \hline
	P\_1000 & 0.0221 \\ \hline
    \end{tabular}
    \caption{Text}    
    }
    \hfill
    \parbox{.45\linewidth}{
    \begin{tabular}{ | l | l | }
    \hline
    Metric & Score \\ \hline
	num\_q & 48 \\ \hline
	num\_ret & 2850 \\ \hline
	num\_rel & 6708 \\ \hline
	num\_rel\_ret & 1040 \\ \hline
	map & 0.5468 \\ \hline
	gm\_map & 0.3540 \\ \hline
	Rprec & 0.5493 \\ \hline
	bpref & 0.5865 \\ \hline
	recip\_rank & 0.9578 \\ \hline
	iprec\_at\_recall\_0.00 & 0.9735 \\ \hline
	iprec\_at\_recall\_0.10 & 0.8409 \\ \hline
	iprec\_at\_recall\_0.20 & 0.6636 \\ \hline
	iprec\_at\_recall\_0.30 & 0.5884 \\ \hline
	iprec\_at\_recall\_0.40 & 0.5441 \\ \hline
	iprec\_at\_recall\_0.50 & 0.5112 \\ \hline
	iprec\_at\_recall\_0.60 & 0.5016 \\ \hline
	iprec\_at\_recall\_0.70 & 0.4751 \\ \hline
	iprec\_at\_recall\_0.80 & 0.4540 \\ \hline
	iprec\_at\_recall\_0.90 & 0.3674 \\ \hline
	iprec\_at\_recall\_1.00 & 0.2521 \\ \hline
	P\_5 & 0.9000 \\ \hline
	P\_10 & 0.8396 \\ \hline
	P\_15 & 0.7972 \\ \hline
	P\_20 & 0.7469 \\ \hline
	P\_30 & 0.6514 \\ \hline
	P\_100 & 0.2158 \\ \hline
	P\_200 & 0.1083 \\ \hline
	P\_500 & 0.0433 \\ \hline
	P\_1000 & 0.0217 \\ \hline
    \end{tabular}
    \caption{Tag}
    }
\end{table}

% query
% trec_eval results

\begin{table}[htb]
    \parbox{.45\linewidth}{
    \begin{tabular}{ | l | l | }
    \hline
    Metric & Score \\ \hline
	num\_q & 48 \\ \hline
	num\_ret & 9353 \\ \hline
	num\_rel & 6708 \\ \hline
	num\_rel\_ret & 1559 \\ \hline
	map & 0.6400 \\ \hline
	gm\_map & 0.4809 \\ \hline
	Rprec & 0.6428 \\ \hline
	bpref & 0.6531 \\ \hline
	recip\_rank & 0.9653 \\ \hline
	iprec\_at\_recall\_0.00 & 0.9924 \\ \hline
	iprec\_at\_recall\_0.10 & 0.9317 \\ \hline
	iprec\_at\_recall\_0.20 & 0.8617 \\ \hline
	iprec\_at\_recall\_0.30 & 0.7472 \\ \hline
	iprec\_at\_recall\_0.40 & 0.6434 \\ \hline
	iprec\_at\_recall\_0.50 & 0.6177 \\ \hline
	iprec\_at\_recall\_0.60 & 0.5445 \\ \hline
	iprec\_at\_recall\_0.70 & 0.5077 \\ \hline
	iprec\_at\_recall\_0.80 & 0.4859 \\ \hline
	iprec\_at\_recall\_0.90 & 0.4062 \\ \hline
	iprec\_at\_recall\_1.00 & 0.3854 \\ \hline
	P\_5 & 0.9292 \\ \hline
	P\_10 & 0.8750 \\ \hline
	P\_15 & 0.8250 \\ \hline
	P\_20 & 0.7781 \\ \hline
	P\_30 & 0.7090 \\ \hline
	P\_100 & 0.3000 \\ \hline
	P\_200 & 0.1613 \\ \hline
	P\_500 & 0.0648 \\ \hline
	P\_1000 & 0.0325 \\ \hline
    \end{tabular}
    \caption{Query}
    }
    \parbox{.45\linewidth}{
    \begin{tabular}{ | l | l | }
    \hline
    Metric & Score \\ \hline
	num\_q & 36 \\ \hline
	num\_ret & 2323 \\ \hline
	num\_rel & 5009 \\ \hline
	num\_rel\_ret & 695 \\ \hline
	map & 0.5649 \\ \hline
	gm\_map & 0.3587 \\ \hline
	Rprec & 0.5536 \\ \hline
	bpref & 0.5784 \\ \hline
	recip\_rank & 0.9504 \\ \hline
	iprec\_at\_recall\_0.00 & 0.9661 \\ \hline
	iprec\_at\_recall\_0.10 & 0.8252 \\ \hline
	iprec\_at\_recall\_0.20 & 0.7426 \\ \hline
	iprec\_at\_recall\_0.30 & 0.6181 \\ \hline
	iprec\_at\_recall\_0.40 & 0.5903 \\ \hline
	iprec\_at\_recall\_0.50 & 0.5852 \\ \hline
	iprec\_at\_recall\_0.60 & 0.5521 \\ \hline
	iprec\_at\_recall\_0.70 & 0.5036 \\ \hline
	iprec\_at\_recall\_0.80 & 0.4212 \\ \hline
	iprec\_at\_recall\_0.90 & 0.3263 \\ \hline
	iprec\_at\_recall\_1.00 & 0.1833 \\ \hline
	P\_5 & 0.8722 \\ \hline
	P\_10 & 0.8194 \\ \hline
	P\_15 & 0.7630 \\ \hline
	P\_20 & 0.6875 \\ \hline
	P\_30 & 0.5593 \\ \hline
	P\_100 & 0.1931 \\ \hline
	P\_200 & 0.0965 \\ \hline
	P\_500 & 0.0386 \\ \hline
	P\_1000 & 0.0193 \\ \hline
    \end{tabular}
    \caption{Relevance Assessment}
    }
\end{table}

These results are visualised in Figure \ref{}

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/task2-confusion-matrix}
    \caption{Visualisation of confusion matrix as a heat map}
    \label{fig:manual-result}
\end{figure}
