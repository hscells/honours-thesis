\chapter{Conclusions}

This chapter summarises and concludes the work that has been done in this thesis and discusses the direction for future work. The contribution of work is summarised by answering each of the research questions. Future work looks at areas of the research that can be expanded and improved.

\section{Summary of Contribution}

\textbf{How can annotations for a collection of lifelog images be evaluated?}

A gold standard annotation does not exist for these lifelog images: individual comparison of annotations and annotation types is not possible. Instead, an alternative extrinsic evaluation methodology is considered. The problem of evaluating image annotations is cast to that of searching for lifelog images based on the collected annotations. In this way image search results are used as a proxy to estimate the quality of the annotations themselves. This extrinsic evaluation method is also the method chosen for the NTCIR-12 lifelog semantic access task. The focus of this research is to investigate the best methodology for annotating images for use in a text-based information retrieval system. The other meta data associated with lifelogs is ignored, in preference of focusing the attention on the images. Supplementary data such as location, time, activity and personal health (heart rate, blood pressure, etc.) are important in pinpointing moments in time, however this research is focused on what types of annotations work best for textual search. Without meaningful annotations the supplementary information becomes useless when performing typical text-based image retrieval. Knowledge of the retrieval effectiveness of each annotation methodology within the context of a text-based image retrieval system allows the research to concentrate on tuning retrieval models, automatically captioning images, and incorporating the aforementioned meta data. 

\textbf{What methods could be used to annotate lifelog images and what is the retrieval effectiveness of annotations collected with these methods?}

Four annotation methodologies are chosen for annotation. Of the selected types of annotations: text, tags, queries, and relevance assessment -- the query annotation is the most effective in a text-based image retrieval search task. In fact, not only is the query annotation methodology the most effective, it is the cheapest to collect in that the average time to collect annotations of this type is the lowest of all four methodologies. The second most effective annotation methodology for lifelog images are relevance assessments, however these are time consuming to collect and require prior selection of concepts to judge. The textual and tag annotations have about the same retrieval effectiveness; the difference is that the textual annotations take double the time on average to annotate. The retrieval effectiveness of the annotations is slightly improved when combining all four together, particularly at high recall.

\textbf{How do annotations generated by current state-of-the-art automatic image captioning techniques compare to the effectiveness of manual annotations?}

The effectiveness of annotations generated by one particular state-of-the-art automatic captioning technique is not comparable to the effectiveness of manual annotations. The expectation of providing more training data in combination with some tuning is that the effectiveness will improve. Current results show, however, that the query annotations still retrieve the most number of images overall. In addition, combining the annotations increases the overall effectiveness, especially at low recall.

\section{Future Work}

\textbf{Moment Annotation}

The most important change to make if continuing this research is to segment the collection of images into moments rather than individual images. This would allow many more images to be annotated in a smaller amount of time and cover more edge cases. New techniques are required to sample images into moments rather than individually, and the image annotation interfaces must be updated to reflect this. 

Adding and removing images from each moment in the annotation interfaces is necessary (the sampling process is not likely to perfectly capture all relevant images in a moment i.e. there could be outliers on the edges of the moment). One thing that needs further investigation is whether to overlap moments with each other. Is it sensible to annotate images more than once if they appear in moments which overlap?

\textbf{Automatic Captioning}

Automatic captioning tools seem to work very well in other domains and it is unfortunate that this research could not exploit them. The number of annotations used as training data is the most likely reason this research could not generate captions/annotations effectively. Although diverse in moments and objects, there are still less than 100,000 images in total, with less than 20,000 of these images considered `relevant'. Many topics have less than 100 relevant images so using a data set with more relevant images may improve how images are captioned. 

The retrieval system itself can also be expanded to combine TBIR and CBIR systems. There has been recent work done to generate images from textual descriptions by Reed et. al~\cite{reed2016generative}; however it looks limited to a small number of categories and the resolution of the generated images are low. It is not difficult to imagine a system which generates images from a query and retrieves using an image similarity technique.

\textbf{Image sampling}

Another line of future work is to investigate other image similarity measures (such as those used by the LEMoRe team ~\cite{de40lemore} at NTCIR-12) to possibly produce a more uniformly distributed sample set. These similarity measures can be used in combination with a better clustering algorithm. Rather than focus on clustering sets of visually similar images, the result of sampling in future work should produce sequences of moments. Further, annotating moments and the affect sampling moments has on the retrieval effectiveness is yet to be seen.

% There has been some work on generating images based on textual descriptions, but it looks limited to small collections of images, and does not seem to be able to produce large resolution images. In the future, search may be able to be performed by image description->image generation->image matching \todo{this https://github.com/paarthneekhara/text-to-image and the paper should be cited here}
 
% Current state of the art automatic image captioning algorithms~\cite{karpathy2015deep} allow many images to be textually annotated with some level of accuracy. 
 
% The collection of images used is still relatively small. Although diverse in moments and objects, there are still less than 100k images. A large-scale test collection in the magnitude of hundreds of thousands of images to a million or more images could offer better results. \todo{I have read some discussions on line about this but I am yet to find any papers/formal research on this, as well as the exact number of images}. Results will only get more accurate because there is a more diverse range of events happening in the collection.

% There are four annotation types under investigation. Each one is very different to the last. There have been no studies into the most effective methodology or set of methodologies for annotating lifelog images. It would be nice to investigate more than four, however due to time constraints this is inconceivable.