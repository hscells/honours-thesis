\chapter{Conclusions}

\section{Contribution Summaries}

\textbf{How can annotations for a collection of lifelog images be evaluated?}

\textbf{What are the possible ways lifelog images can be annotated and what is their retrieval effectiveness?
}

\textbf{How do annotations generated by current state-of-the-art automatic image captioning techniques compare to the effectiveness of manual annotations?}


\section{Future Work}

\textbf{Image Interfaces}

The most important change to make if continuing this research is to segment the collection of images into moments rather than individual images. This would allow many more images to be annotated in a smaller amount of time and cover more edge cases. New techniques are required to sample images into moments rather than individually, and the image annotation interfaces must be updated to reflect this. 

Adding and removing images from each moment in the annotation interfaces is necessary (the sampling process is not likely to perfectly capture all relevant images in a moment i.e there could be outliers on the edges of the moment). One thing that needs further investigation is whether to overlap moments with each other. Is it sensible to annotate images more than once if they appear in moments which overlap?

\textbf{Automatic Captioning}

Automatic captioning tools seem to work very well in other domains and it is unfortunate that this research could not exploit them. The number of annotations used as training data is the most likely reason this research could not generate captions effectively. Although diverse in moments and objects, there are still less than 100,000 images in total, with less than 20,000 of these images considered `relevant'. Many topics have less than 100 relevant images so using a data set with more relevant images may improve how images get captioned. 

The retrieval system itself can also be expanded up to combine TBIR and CBIR systems. There has been recent work done to generate images from textual descriptions by Reed et. al~\cite{reed2016generative}; however it looks limited to small categories and the resolution of the generated images are low. It is not difficult to imagine a system which generates images from a query and retrieves using an image similarity technique. As of writing this, the idea is only a pipe dream; the future of automatic captioning and image retrieval of lifelogs is bright.

\textbf{Image sampling}

Given more time, it would be worthwhile investigating other image similarity measures (such as those used by the LEMoRe team ~\cite{de40lemore} at NTCIR-12) to possibly produce a more uniformly distributed sample set.

% There has been some work on generating images based on textual descriptions, but it looks limited to small collections of images, and does not seem to be able to produce large resolution images. In the future, search may be able to be performed by image description->image generation->image matching \todo{this https://github.com/paarthneekhara/text-to-image and the paper should be cited here}
 
% Current state of the art automatic image captioning algorithms~\cite{karpathy2015deep} allow many images to be textually annotated with some level of accuracy. 
 
% The collection of images used is still relatively small. Although diverse in moments and objects, there are still less than 100k images. A large-scale test collection in the magnitude of hundreds of thousands of images to a million or more images could offer better results. \todo{I have read some discussions on line about this but I am yet to find any papers/formal research on this, as well as the exact number of images}. Results will only get more accurate because there is a more diverse range of events happening in the collection.

% There are four annotation types under investigation. Each one is very different to the last. There have been no studies into the most effective methodology or set of methodologies for annotating lifelog images. It would be nice to investigate more than four, however due to time constraints this is inconceivable.