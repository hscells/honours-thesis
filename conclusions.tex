\chapter{Conclusions}

\section{Contribution Summaries}

\textbf{How can annotations for a collection of lifelog images be evaluated?}

A gold standard annotation does not exist for these lifelog images: individual comparison of annotations is not possible. An ad-hoc TREC-style is instead the technique for evaluating annotations. This extrinsic evaluation method is also the method chosen for the NTCIR-12 lifelog semantic access task. The focus of this research is to investigate the best methodology for annotating images for use in a text-based information retrieval system. The other meta data associated with lifelogs is ignored, in preference of focusing the attention on the images. Knowledge of the retrieval effectiveness of each annotation methodology within the context of a text-based image retrieval system allows the research to concentrate on tuning retrieval models, automatically captioning images, and incorporating the aforementioned meta data. 

\textbf{What are the possible ways lifelog images can be annotated and what is their retrieval effectiveness?
}

Four annotation methodologies are chosen for determining their retrieval effectiveness. Of the selected types of annotations: text, tags, queries, and relevance assessment -- the query annotation is the most effective in a text-based image retrieval search task. In fact, not only is the query annotation methodology the most effective, it is the cheapest to collect in that the average time to collect one type of this annotation is the lowest of all four methodologies. The second most effective annotation methodology for lifelog images are relevance assessments, however these are time consuming to collect and require prior selection of concepts to judge. The textual and tag annotations have about the same retrieval effectiveness; the difference is that the textual annotations take double the time on average to annotate. The retrieval effectiveness of the annotations is slightly improved when combining all four together, particularly at high recall.

\textbf{How do annotations generated by current state-of-the-art automatic image captioning techniques compare to the effectiveness of manual annotations?}

The effectiveness of annotations generated by one particular state-of-the-art automatic captioning technique is not comparable to the effectiveness of manual annotations. The expectation of providing more training data in combination with some tuning is that the effectiveness will improve. Current results show, however, that the query annotations still retrieve the most number of images overall. Again, combining the annotations increases the overall effectiveness, especially at low recall.

\section{Future Work}

\textbf{Moment Annotation}

The most important change to make if continuing this research is to segment the collection of images into moments rather than individual images. This would allow many more images to be annotated in a smaller amount of time and cover more edge cases. New techniques are required to sample images into moments rather than individually, and the image annotation interfaces must be updated to reflect this. 

Adding and removing images from each moment in the annotation interfaces is necessary (the sampling process is not likely to perfectly capture all relevant images in a moment i.e there could be outliers on the edges of the moment). One thing that needs further investigation is whether to overlap moments with each other. Is it sensible to annotate images more than once if they appear in moments which overlap?

\textbf{Automatic Captioning}

Automatic captioning tools seem to work very well in other domains and it is unfortunate that this research could not exploit them. The number of annotations used as training data is the most likely reason this research could not generate captions effectively. Although diverse in moments and objects, there are still less than 100,000 images in total, with less than 20,000 of these images considered `relevant'. Many topics have less than 100 relevant images so using a data set with more relevant images may improve how images get captioned. 

The retrieval system itself can also be expanded up to combine TBIR and CBIR systems. There has been recent work done to generate images from textual descriptions by Reed et. al~\cite{reed2016generative}; however it looks limited to small categories and the resolution of the generated images are low. It is not difficult to imagine a system which generates images from a query and retrieves using an image similarity technique. As of writing this, the idea is only a pipe dream; the future of automatic captioning and image retrieval of lifelogs is bright.

\textbf{Image sampling}

Given more time, it would be worthwhile investigating other image similarity measures (such as those used by the LEMoRe team ~\cite{de40lemore} at NTCIR-12) to possibly produce a more uniformly distributed sample set.

% There has been some work on generating images based on textual descriptions, but it looks limited to small collections of images, and does not seem to be able to produce large resolution images. In the future, search may be able to be performed by image description->image generation->image matching \todo{this https://github.com/paarthneekhara/text-to-image and the paper should be cited here}
 
% Current state of the art automatic image captioning algorithms~\cite{karpathy2015deep} allow many images to be textually annotated with some level of accuracy. 
 
% The collection of images used is still relatively small. Although diverse in moments and objects, there are still less than 100k images. A large-scale test collection in the magnitude of hundreds of thousands of images to a million or more images could offer better results. \todo{I have read some discussions on line about this but I am yet to find any papers/formal research on this, as well as the exact number of images}. Results will only get more accurate because there is a more diverse range of events happening in the collection.

% There are four annotation types under investigation. Each one is very different to the last. There have been no studies into the most effective methodology or set of methodologies for annotating lifelog images. It would be nice to investigate more than four, however due to time constraints this is inconceivable.