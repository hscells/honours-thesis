\chapter{Background}

\input{litreview.tex}

\section{Findings From NTCIR}
The NTCIR-12 lifelogging latent semantic access pilot task consisted of four participants contributing to the automatic retrieval component and one participant in the interactive retrieval component. The highest performing automatic team, LIG-MRIM, used computer vision to classify images and not rely on the visual concepts distributed with the task. The interactive team (LEMoRe) outperformed all other automatic teams, but this is expected as is generally the case with tasks that contain both automatic and interactive components. The other three automatic teams are VTIR, III\&CYUT and QUT (the preliminary work done for this research).

LIG-MRIM~\cite{safadilig2016ligmrim} used dynamic convolutional neural networks and a multi-class support vector machine (MSVM) in order to classify images. Visual indexing is composed of two parts: three deep convolutional neural network models (AlexNet, GoogleNet and Visual Geometry Grouping (VGG)) process each image. The output is normalised and has principal component analysis (PCA) performed on it. These outputs are then concatenated together. The same normalisation and PCA process is repeated and fed into the MSVM. The output of this is concatenated with the VGG data. The second part involves temporally naming times of the day in order to attempt to extract semanitic meaning from times of the day. While this team submitted runs that fit the definition of automatic for the task, queries were generated manually from the topics by an expert.

III\&CYUT~\cite{lin2016image} used a traditional textual based approach to lifelog retrieval. A skipgram word embedding obtained with word2vec\cite{mikolov2013word2vec} was computed for the visual concepts dis- tributed with the dataset. These embeddings were then used in an attempt to add more semantic meaning to images. Specifically, the embeddings were used within a document expansion process, resulting in a translation language model~\cite{zuccon2015integrating}. Query expansion was also used on every keyword.

The VTIR team~\cite{xia2016vtir} attempted to exploit location metadata associtaed with the images. To this end 3,000 random images were labelled against a rich semantic location ontology. More concepts were utilised by applying the WordNet database to find cognitive synonyms. Despite the additional annotations, this system failed to provide good retrieval effectiveness.

Finally, the LEMoRe~\cite{de40lemore} team, which used an interactive approach, combined existing technologies and methodologies in order to develop a search engine. Colour correlogram, edge histogram, joint composite descriptor and pyramid histogram of oriented graphics are used by the image retrieval system as features to retrieve images. Both a novice and an expert used the system to produce runs.

The results from this pilot task offer a promising glimpse into the future of searching lifelog images. LIG-MRIM shows that automatic methods for annotating lifelog images can result in decent text-based image retrieval effectiveness. The teams that did not perform so well (including the QUT team) offer insight into areas of research to avoid.