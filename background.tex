\chapter{Background}

This chapter presents a background on the current state of searching lifelog images. This is done through a review of the literature and findings from the NTCIR-12 lifelog semantic access task.

\input{litreview.tex}

\section{Findings From NTCIR}
The NTCIR-12 lifelogging latent semantic access pilot task consisted of four participants contributing to the automatic retrieval component and one participant in the interactive retrieval component. The highest performing automatic team, LIG-MRIM, used computer vision to classify images and not rely on the visual concepts distributed with the task. The interactive team (LEMoRe) outperformed all other automatic teams, but this is expected as is generally the case with tasks that contain both automatic and interactive components. The other three automatic teams are VTIR, III\&CYUT and QUT (the preliminary work done for this research).

LIG-MRIM~\cite{safadilig2016ligmrim} used dynamic convolutional neural networks and a multi-class support vector machine (MSVM) in order to classify images. Visual indexing is composed of two parts: three deep convolutional neural network models (AlexNet, GoogleNet and Visual Geometry Grouping (VGG)) process each image. The output is normalised and has principal component analysis (PCA) performed on it. These outputs are then concatenated together. The same normalisation and PCA process is repeated and fed into the MSVM. The output of this is concatenated with the VGG data. The second part involves temporally naming times of the day in order to attempt to extract semanitic meaning from times of the day. While this team submitted runs that fit the definition of automatic for the task, queries were generated manually from the topics by an expert.

III\&CYUT~\cite{lin2016image} used a traditional textual based approach to lifelog retrieval. A skipgram word embedding obtained with word2vec\cite{mikolov2013word2vec} was computed for the visual concepts dis- tributed with the dataset. These embeddings were then used in an attempt to add more semantic meaning to images. Specifically, the embeddings were used within a document expansion process, resulting in a translation language model~\cite{zuccon2015integrating}. Query expansion was also used on every keyword.

The VTIR team~\cite{xia2016vtir} attempted to exploit location metadata associtaed with the images. To this end 3,000 random images were labelled against a rich semantic location ontology. More concepts were utilised by applying the WordNet database to find cognitive synonyms. Despite the additional annotations, this system failed to provide good retrieval effectiveness.

Finally, the LEMoRe~\cite{de40lemore} team, which used an interactive approach, combined existing technologies and methodologies in order to develop a search engine. Colour correlogram, edge histogram, joint composite descriptor and pyramid histogram of oriented graphics are used by the image retrieval system as features to retrieve images. Both a novice and an expert used the system to produce runs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{graphs/ntcir-pr-curve}
    \caption{Precision-recall curves for the four LSAT teams}
    \label{fig:ntcir-results}
\end{figure}

The results from this pilot task offer a promising glimpse into the future of searching lifelog images. Figure \ref{fig:ntcir-results} presents the best run from each of the teams that participated in NTCIR-12 LSAT. LIG-MRIM shows that automatic methods for annotating lifelog images can result in decent text-based image retrieval effectiveness. The teams that do not perform well (including the QUT team) offer insight into areas of research to avoid. 

The difference in retrieval effectiveness between LIG-MRIM and the other three teams is highly likely due to the annotations of the images. The task provides teams with automatically generated annotations for the images distributed with the task. These annotations are generated using a previously state-of-the-art captioning framework, CAFFE~\cite{jia2014caffe}. The problem lies within the fact that a CAFFE model is trained on a data set that does not align with the lifelog images. Three of the teams use these annotations in their systems; however LIG-MRIM generate their own annotations. This indicates that no matter how well tuned and suited a text-based image retrieval system is to lifelog images, poor textual representations for images can significantly impact the retrieval effectiveness.