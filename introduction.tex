\chapter{Introduction}

What if you had hundreds of thousands of images of your everyday activities collected through a lifelogging device and you wanted to search for meaningful events in this collection? This is a novel area of research within the domain of information retrieval~\cite{gurrin2014lifelogging}. A range of consumer products allow anyone to capture their daily activities and it is becoming increasingly popular~\cite{gurrin2014lifelogging}\cite{van2014future}\cite{askoxylakis2011log}. The devices are typically sold under the umbrella term of "lifelogging cameras". Like blogging and vlogging before it, lifelogging is the next step in this series of life event recording devices. Not only is this an area of research for consumer products, but also has applications for security and policing services where there is a need to search the cameras which are already being worn.

Preliminary research has provided an insight into how difficult searching lifelogging images is. Typical search methodologies (TBIR and CBIR) are not applicable and, until recently, there have been no freely available collections. Currently, there are no machine learning approaches which can consistently and reliably discover concepts and objects from images. There are multiple ways of annotating images automatically, however this research is primarily focused on investigating the performance of manual annotations. By investigating a number of annotation methodologies and discovering which one is the most appropriate for text based information retrieval on lifelog images, an answer for the best way of annotating a collection of lifelog images. These annotations are made available to the wider research community to further the research into the field of lifelog search. Supplementary data such as location, time, activity and personal health (heart rate, blood pressure, etc.) are important in pinpointing moments in time, however this research will focus on what types of annotations work best for textual search. Without meaningful annotations the supplementary information becomes useless when performing typical text-based search.

This text-based search of images is most commonly referred to as TBIR (text-based image retrieval). The alternative, CBIR (content-based image retrieval) does away with textual features and relies on the visual features of an image; for example colour, shape or texture. The benefit of TBIR over CIBR is that is supports image retrieval based on high-level textual semantic concepts~\cite{hartvedt2010using}. While it is believed by Hartvedt~\cite{hartvedt2010using} that neither TBIR nor CBIR alone are optimally suited to support context focused image retrieval, the different annotation methodologies aims to prove or disprove this belief.\todo{look at CBIR}

Collecting annotations is important, but determining which methodology is the best is a larger challenge. Evaluation for images generally rely on gold standard annotations, none of which exist for lifelog images. The problem becomes clear - these standard ways of evaluation are not applicable to lifelog annotations unless a reference exists. A framework involving TREC-style runs is developed for evaluating arbitrary types of annotations for lifelog images. The pipeline for evaluating lifelog images and their annotations and the data produced is an important and meaningful contribution to the research community. 

Due to the sheer number of annotations needed to collect in such a short amount of time (six months) and less than five annotators, automatic image captioning will also be investigated as part of this research. An image captioning system will be trained on the manually collected annotations in order to annotate the entire collection of lifelog images. The NTCIR-12 lifelog semantic access task (LSAT) was a pilot task which asked teams to search and rank lifelog images. This task made a collection of images available for research purposes. The results from this research are compared with the results from the NTCIR-12 LSAT to see if the methods of annotating improve over the performance of previous attempts. Runs are produced in the same format as the runs from the NTCIR-12 LSAT and evaluated under the same settings.

\section{Aims and Objectives}

Four annotation methodologies have been chosen as a basis for this research. Each methodology will involve the collection of annotations through some interface and evaluation. The aim of the project is to determine, through TREC-style evaluation, the best methodology for annotating lifelog images. The four methodologies under investigation are: 

\begin{enumerate}
    \item Tags --- Sets of keywords that describe objects and semantics of an image. Tags are chosen from a user-defined, non persistent vocabulary. These are similar to what one would expect to see on Flickr and other similar sites.
    \item Textual Descriptions --- Descriptive long-form annotations that contain semantic meaning. These are similar to the contents of documents such as web pages or news papers.
    \item Relevance Assessments --- Images are scored on a 0-10 scale by how relevant the image relates to a given concept. These are more purposely obtained, similar to \todo{editorial(?)} efforts by search engine companies.
    \item Reverse Queries --- Queries are formulated for a given search result listing. Annotators are asked to provide a query that they think would result in the image being returned by a typical search engine. These are more akin to what one would expect to see in query logs.
\end{enumerate}

In order to collect annotations, an interface will need to be designed and developed. This will involve a website that will facilitate the collection of these annotations. Once a suitable number of annotations have been collected, each methodology will be evaluated. This will involve a technique, whereby the evaluation of annotations is embedded within a search task (similar to topic modelling). The evaluation will be generic, in that \textit{any} type of annotation can be submitted for evaluation. It is hoped that the methodology can be applied or built upon for future work in the area of searching lifelog images.

This research also aims to produce two meaningful contributions to the lifelog research community. Recent results from the NTCIR-12 conference indicate lifelog search engines which incorporate image processing in some way perform much better than simply using annotations alone. A new collection of \textit{well} annotated images as well as training data for image classification systems would hopefully boost the performance for everyone researching lifelog search engines. Secondly, anyone hoping to produce more annotations for a collection of lifelog images may wish to know how effective their annotation methodology is and how well it performs with respect to others. An easy to use tool for evaluation of lifelog image annotations is a necessity for this new area of research.

Finally, automatic image captioning will be investigated using the manual annotations that will be collected. Here, a state-of-the-art image captioning tool will be used to annotate images using the annotations that have already been collected. The end result will be a fully annotated collection of lifelog images that can be searched and distributed also. Previous attempts at using textual descriptions as annotations~\cite{scells2016qut} gave poor, but hopeful results. It is hoped that by simply adding more annotations, the performance of previous search engines can be improved.

\section{Research Gaps}

The literature reviewed has revealed two gaps in the research. Annotation of lifelog images and the evaluation of them is not a standard process. Through preliminary research, a number of techniques have been investigated which are applicable to annotating images. A number of textual summarisation evaluation methods have also been discovered, however none of these are capable of evaluation without a ground truth or gold standard. Through researching the following two questions, it is hoped that these gaps in the research can be closed. According to an examination of evaluation in information retrieval systems~\cite[p. 24]{sanderson2010test}, there has been very little work done to evaluate how good a test collection is. Furthermore, finding all relevant documents to build topics is still the accepted approach for creating a test collection~\cite{cooper1973selecting}.

\section{Research Questions}

Two research questions have been identified by looking at previous research efforts in the literature. 

\textbf{Research Question 1:} How can annotations for a collection of lifelog images be evaluated? It is important that the annotations of images are accurate and of a high quality. Low quality annotations will lead to bad evaluation results. Evaluation will be performed without using a gold standard annotation, since this does not exist. In this way, this research will investigate a novel way of evaluating annotations for lifelog images. The most promising way to do this is through extrinsic evaluation, whereby annotations are evaluated by determining the performance of a larger system as a whole with respect to each annotation methodology. In order to keep evaluation fair, a number of baseline retrieval models will be tested to weigh out the differences between them.

\textbf{Research Question 2:} What are the possible ways lifelog images can be annotated and what is their retrieval effectiveness? There are many state of the art solutions for automatically summarising the contents of images, however this project will involve manual annotations. This is due to the fact that there does not exist any training data specifically for lifelog images. This makes it challenging to train a model that can identify what is in a lifelog image. It is thought that by manually annotating images, a well formed collection of annotated lifelog images can be produced, as well as data which machine learning and computer vision algorithms can exploit.
