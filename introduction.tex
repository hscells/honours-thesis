\chapter{Introduction}

What if you had hundreds of thousands of images of your everyday activities collected through a lifelogging device and you wanted to search for meaningful events in this collection? Searching through large data sets of lifelogging images for insights about daily activities is a fast emerging area of research within the information retrieval domain~\cite{gurrin2014lifelogging}. A range of consumer products allow anyone to capture their daily activities and it is becoming increasingly popular~\cite{gurrin2014lifelogging}\cite{van2014future}\cite{askoxylakis2011log}. These devices are typically sold under the umbrella term of `lifelogging cameras'. Like precursors blogging and vlogging, lifelogging is the next step in this series of life event recording devices. Lifelogging not only benefits consumers who wish to document their lives, but also has applications for security and policing services where there is a need to search the images recorded through body cameras that are already being worn.

Preliminary research has provided an insight into how difficult searching lifelogging images is~\cite{scells2016qut}. The text-based search of images is most commonly referred to as text-based image retrieval (TBIR). This research considers text based search solutions for lifelogging search. The alternative, content-based image retrieval (CBIR) does away with textual features and relies on the visual features of an image; for example colour, shape or texture, but generally assumes querying is through an example query image. In a study by Hartvedt~\cite{hartvedt2010using}, it was found that the benefit of TBIR over CIBR is that is supports image retrieval based on high-level textual semantic concepts. This claim is supported by Shao et.al~\cite{medical2004shao} who notes that TBIR exploits semantic information in the text associated with images. CBIR is not applicable as images are searched with textual queries only and, as stated before, CBIR assumes querying occurs through example images. TBIR is the predominant approach for image retrieval and most commercial search engines rely on this scheme~\cite{escalante2007towards}. TBIR intuitively relies heavily on quality annotations; if annotations have grammatical and spelling errors or do not correctly describe the content of the image (no semantic context) then the retrieval effectiveness is degraded. The requirements of a `good' textual document in, for example, web search, also applies to annotations of images.

Currently, no machine learning approach can consistently and reliably generate concepts (or annotations) from lifelog images due to the difference between the training data and the data contained in lifelogs. There are multiple methods of annotating images automatically, however this research is primarily focused on investigating the performance of manual annotations. A number of annotation methodologies are tested to find the most appropriate type of annotation for text based image retrieval of lifelog images.

Collecting annotations is important, but another important challenge is to determine which annotation methodology is the best for text-based image retrieval. Evaluation of images generally relies on gold standard annotations, none of which exist for lifelog images. The problem becomes clear - the standard ways of evaluation are not applicable to lifelog annotations unless a reference exists. A framework involving TREC-style\footnote{\url{http://trec.nist.gov/}} runs is used for evaluating arbitrary types of annotations associated with lifelog images. The quality of the annotations is evaluated by means of using annotations to inform the image representation in a TBIR system for lifelog search. The pipeline for evaluating lifelog images and their annotations and the data produced is an important and meaningful contribution of the thesis. 

Due to the sheer volume of annotations required to collect in the short time frame allowed by this thesis project and by the limited resources available for manual annotation, automatic image captioning is also investigated as part of this research. In this thesis, an image captioning system is trained on the manually collected annotations in order to annotate the entire collection of lifelog images. The collection of image is obtained form the the NTCIR-12 lifelog semantic access task (LSAT). The NTCIR-12 LSAT is a pilot task which has research teams retrieve and rank lifelog images using textual queries. The results from this research can be compared with the results from the NTCIR-12 LSAT to determine if the methods for image annotations investigated here improve over the performance of previous attempts.

\section{Aims and Objectives}

Four annotation methodologies are chosen as a basis for this research. Each methodology involves the collection of annotations through some interface and evaluation. The aim of the project is to determine the best methodology for annotating lifelog images. The four methodologies under investigation are: 

\begin{enumerate}
    \item Tags --- Sets of keywords that describe objects and semantics of an image. Tags are chosen from a user-defined, non persistent vocabulary. These are similar to what one would expect to see on Flickr and other similar sites.
    \item Textual Descriptions --- Descriptive long-form annotations that contain semantic meaning. These are similar to the contents of documents such as web pages or news papers.
    \item Relevance Assessments --- Images are scored on a 0-10 scale by how relevant the image relates to a given concept. These are more purposely obtained, similar to editorial efforts by commercial search engine companies.
    \item Reverse Queries --- Queries are formulated for a given search result listing. Annotators are asked to provide a query that they think would result in the image being returned by a typical search engine. These are more akin to what one would expect to see in query logs.
\end{enumerate}

To collect these annotations, interfaces are developed to facilitate the entering and recording of the annotations. Once a suitable number of annotations have been collected, each methodology is evaluated. The evaluation strategies available are explored as part of this research.

This research also aims to produce two meaningful contributions to the lifelog research community. Recent results from the NTCIR-12 conference indicate lifelog search engines which incorporate image processing in some way perform much better than simply using annotations alone~\cite{safadilig2016ligmrim}. A new higher quality collection of annotations for lifelog images as well as training data for image classification systems is expected to boost the performance for everyone researching lifelog search engines: the annotations are released to the research community. This directs future efforts for collecting and evaluating annotations and can act as a comparison for future lifelog annotations.

Finally, automatic image captioning is investigated. Here, a state-of-the-art image captioning pipeline generates captions for images using the manually collected annotations as training data. The end result is a fully annotated collection of lifelog images. Previous attempts at using textual descriptions as annotations~\cite{scells2016qut} resulted in poor, but optimistic results. 

Of the four annotation methodologies selected for investigation, a recommendation is made on the most effective of these and the results of the automatically generated captions are compared to the results of the manually collected annotations. 

\section{Research Gaps}

Two gaps are identified in the research through reviewing the literature. Annotation of lifelog images and their evaluation is not a standard process. Through preliminary research, a number of techniques have been investigated which are applicable to annotating images. A number of textual summarisation evaluation methods have also been considered, however none of these are capable of evaluation without a ground truth or gold standard. According to an examination of evaluation in information retrieval~\cite[p. 24]{sanderson2010test}, there has been very little work done to evaluate how good a test collection is. Furthermore, finding all relevant documents to build topics is still the accepted approach for creating a test collection~\cite{cooper1973selecting}.

It is currently unknown which annotation methodology most suits a lifelog images for a given TBIR system. This also implies that given a collection of lifelog images, it is unknown which form of annotation methodology for the images results in the best performance. Annotations (in general~\cite{snow2008cheap}) are very expensive to collect in both time and financially, thus determining the most appropriate methodology of annotation prior to collecting annotations is highly important.

\section{Research Questions}

This thesis aims to answer the following research questions:

\textbf{Research Question 1:} How can annotations for a collection of lifelog images be evaluated?

It is important that the annotations of images are accurate and of a high quality. Low quality annotations lead to poor evaluation results. Evaluation is performed without using a gold standard annotation, since this does not exist. In this way, this research uses the ad-hoc TREC-style approach used in the NTCIR-12 lifelog semantic access task. This evaluation methodology is also referred to as extrinsic evaluation, whereby annotations are evaluated by determining the performance of a larger system as a whole with respect to each annotation methodology.

\textbf{Research Question 2:} What methods could be used to annotate lifelog images and what is the retrieval effectiveness of annotations collected with these methods?
% What are the possible ways lifelog images can be annotated and what is their retrieval effectiveness?

There are many state of the art solutions for automatically summarising the contents of images~\cite{karpathy2015deep}\cite{jia2014caffe}\cite{pan2004gcap}, however this project involves measuring the effectiveness of manual annotations. This is due to the fact that training data specifically for lifelog images does not exit. This makes it challenging to train a model that can identify the contents of a lifelog image. In manually annotating images, a well formed collection of annotated lifelog images is produced and machine learning and computer vision algorithms can exploit this.

\textbf{Research Question 3:} RQ3: How do annotations generated by current state-of-the-art automatic image captioning techniques compare to the effectiveness of manual annotations?

The captions generated by machine learning are able to be evaluated in the same way the manual annotations are; the format of the annotations will not differ. The effectiveness of these automatic annotations provides a good comparison to the manually collected annotations and offers insight into how good the manual annotations are as training examples. Unlike the manual annotations, the automatically generated annotations are for the entire collection of lifelog images. 