\chapter{Abstract}

\chapter{Introduction}

What if you had hundreds of thousands of images collected passively throughout the day and you wanted to search for events in this collection? This is a brand new, novel area of research within the domain of information retrieval~\cite{gurrin2014lifelogging}. A range of consumer products allow anyone to capture their daily activities and it is becoming increasingly popular~\cite{gurrin2014lifelogging}\cite{van2014future}\cite{askoxylakis2011log}. The devices are typically sold under the umbrella term of "lifelogging cameras". Like blogging and vlogging before it, lifelogging is the next step in this series of life event recording devices. Not only is this an area of research for consumer products, but also has applications for security and policing services where there is a need to search the cameras which are already being worn by members of these fields.

Preliminary research has provided an insight into how difficult searching lifelogging images is. Typical search methodologies are not applicable and, until recently, there have been no freely available collections. Currently, there are no machine learning approaches which can consistently and reliably discover concepts and objects from images. Since the best option is still to annotate images manually using humans, this is what this research aims to assist with. By investigating a number of annotation methodologies and discovering which one most well suits lifelog images, we hope to provide an answer for the best way of annotating a collection of lifelog images. It is hoped that these annotations can be contributed to the wider research community to further the research into the field of lifelog search. Supplementary data such as location, time, activity and personal health (heart rate, blood pressure, etc.) are important in pinpointing moments in time, however this research will focus on what types of annotations work best for textual search. Without meaningful annotations the supplementary information becomes useless when performing typical text-based search.

This text-based search of images is most commonly referred to as TBIR (text-based image retrieval). The alternative, CBIR (content-based image retrieval) does away with textual features and relies on the visual features of an image; for example colour, shape or texture. The benefit of TBIR over CIBR is that is supports image retrieval based on high-level semantic concepts~\cite{hartvedt2010using}. While it is believed by~\cite{hartvedt2010using} that neither TBIR nor CBIR alone are optimally suited to support context focused image retrieval, the different annotation methodologies aims to prove or disprove this belief.

Collecting annotations is important, but determining which methodology is the best is a larger challenge. Typical textual evaluation strategies rely on a gold standard annotation. The problem becomes clear - these standard ways of evaluation are not applicable to lifelog annotations. There are currently no ways of evaluating arbitrary types of annotations for lifelog image. Designing a system for evaluating lifelog images and their annotations, and not only the types of annotations being researched by this project, but any type of annotation, will also be an important and meaningful contribution to the research community. 

Due to the sheer number of annotations needed to collect in such a short amount of time, automatic image captioning will also be investigated as part of this research. An image captioning system will be trained on the manually collected annotations in order to annotate the entire NTCIR-12 collection of lifelog images. We can then compare the results from this research with the results from the NTCIR-12 lifelog semantic access task and see if the methods of annotating improve the performance of previous attempts.

\section{Aims and Objectives}

Four annotation methodologies have been chosen as a basis for this research. Each methodology will involve the collection of annotations through some interface and evaluation. The aim of the project is to determine, through evaluation, the best methodology for annotating lifelog images. The four methodologies under investigation are: 

\begin{enumerate}
    \item Tags --- Sets of keywords that describe objects and semantics of an image. Tags are chosen from a user-defined, non persistent vocabulary.
    \item Textual Descriptions --- Descriptive long-form annotations that contain semantic meaning.
    \item Relevance Assessments --- Images are scored on a 0-10 scale by how relevant the image relates to a given concept.
    \item Reverse Queries --- Queries are formulated for a given search result listing. Annotators are asked to provide a query that they think would result in the image being returned by a typical search engine.
\end{enumerate}

In order to collect annotations, an interface will need to be designed and developed. This will involve a website that will facilitate the collection of these annotations. Once a suitable number of annotations have been collected, each methodology will be evaluated. This will involve a novel technique, whereby the evaluation of annotations is embedded within a search task (similar to topic modelling). The evaluation will be generic, in that \textit{any} type of annotation can be submitted for evaluation. It is hoped that the methodology can be applied or built upon for future work in the area of searching lifelog images.

This research also aims to produce two meaningful contributions to the lifelog research community. Recent results from the NTCIR-12 conference indicate lifelog search engines which incorporate image processing in some way perform much better than simply using annotations alone. A new collection of \textit{well} annotated images as well as training data for image classification systems would hopefully boost the performance for everyone researching lifelog search engines. Secondly, anyone hoping to produce more annotations for a collection of lifelog images may wish to know how effective their annotation methodology is and how well it performs with respect to others. An easy to use tool for evaluation of lifelog image annotations is a necessity for this new area of research.

Finally, automatic image captioning will be investigated using the manual annotations that will be collected. Here, a state-of-the-art image captioning tool will be used to annotate images using the annotations that have already been collected. The end result will be a fully annotated collection of lifelog images that can be searched and distributed also. Previous attempts at using textual descriptions as annotations~\cite{scells2016qut} gave poor, but hopeful results. It is hoped that by simply adding more annotations, the performance of previous search engines can be improved.

\section{Research Gaps}

The literature reviewed has revealed two gaps in the research. Annotation of lifelog images and the evaluation of them is not a standard process. Through preliminary research, a number of techniques have been investigated which are applicable to annotating images. A number of textual summarisation evaluation methods have also been discovered, however none of these are capable of evaluation without a ground truth or gold standard. Through researching the following two questions, it is hoped that these gaps in the research can be closed. According to an examination of evaluation in information retrieval systems~\cite[p. 24]{sanderson2010test}, there has been very little work done to evaluate how good a test collection is. Furthermore, finding all relevant documents to build topics is still the accepted approach for creating a test collection~\cite{cooper1973selecting}.

\section{Research Questions}

Two research questions have been identified by looking at previous research efforts in the literature. 

\textbf{Research Question 1:} How can annotations for a collection of lifelog images be evaluated? It is important that the annotations of images are accurate and of a high quality. Low quality annotations will lead to bad evaluation results. Evaluation will be performed without using a gold standard annotation, since this does not exist. In this way, this research will investigate a novel way of evaluating annotations for lifelog images. The most promising way to do this is through extrinsic evaluation, whereby annotations are evaluated by determining the performance of a larger system as a whole with respect to each annotation methodology. In order to keep evaluation fair, a number of baseline retrieval models will be tested to weigh out the differences between them.

\textbf{Research Question 2:} What are the possible ways lifelog images can be annotated? There are many state of the art solutions for automatically summarising the contents of images, however this project will involve manual annotations. This is due to the fact that there does not exist any training data specifically for lifelog images. This makes it challenging to train a model that can identify what is in a lifelog image. It is thought that by manually annotating images, a well formed collection of annotated lifelog images can be produced, as well as data which machine learning and computer vision algorithms can exploit.
